
---

# ğŸ“‘ ç›®å½•

* [LoRA (Low-Rank Adaptation) æŠ€æœ¯æ€»ç»“](#lora-low-rank-adaptation-æŠ€æœ¯æ€»ç»“)

  * [1. æ¦‚å¿µ](#1-æ¦‚å¿µ)

    * [1.1 å®šä¹‰](#11-å®šä¹‰)
    * [1.2 ç‰¹ç‚¹](#12-ç‰¹ç‚¹)

  * [2. æ•°å­¦åŸç†](#2-æ•°å­¦åŸç†)

    * [2.1 çº¿æ€§å±‚ä½ç§©è¡¨ç¤º](#21-çº¿æ€§å±‚ä½ç§©è¡¨ç¤º)
    * [2.2 Transformer ä¸­åº”ç”¨](#22-transformer-ä¸­åº”ç”¨)

  * [3. ä¼˜åŠ¿ä¸é€‚ç”¨åœºæ™¯](#3-ä¼˜åŠ¿ä¸é€‚ç”¨åœºæ™¯)

  * [4. LoRA å¾®è°ƒæµç¨‹](#4-lora-å¾®è°ƒæµç¨‹)

    * [4.1 æµç¨‹ç¤ºæ„](#41-æµç¨‹ç¤ºæ„)
    * [4.2 Python ä»£ç ç¤ºä¾‹](#42-python-ä»£ç ç¤ºä¾‹)

  * [5. åœ¨å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„åº”ç”¨](#5-åœ¨å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„åº”ç”¨)

    * [5.1 è¾“å…¥æŠ•å½±å±‚å¾®è°ƒ](#51-è¾“å…¥æŠ•å½±å±‚å¾®è°ƒ)
    * [5.2 è¾“å‡ºæŠ•å½±å±‚å¾®è°ƒ](#52-è¾“å‡ºæŠ•å½±å±‚å¾®è°ƒ)
    * [5.3 LLM ä¸»å¹²å¾®è°ƒ](#53-llm-ä¸»å¹²å¾®è°ƒ)
    * [5.4 å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ](#54-å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ)

  * [6. å®è·µå»ºè®®](#6-å®è·µå»ºè®®)

---

# LoRA (Low-Rank Adaptation) æŠ€æœ¯æ€»ç»“

## 1. æ¦‚å¿µ

**LoRA**ï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§**å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æŠ€æœ¯**ï¼Œç”¨äºåœ¨ä¸ä¿®æ”¹å¤§æ¨¡å‹åŸå§‹æƒé‡çš„æƒ…å†µä¸‹è¿›è¡Œä»»åŠ¡ç‰¹å®šå¾®è°ƒã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šåªè®­ç»ƒ**ä½ç§©çŸ©é˜µ**ä½œä¸ºé€‚é…å™¨æ¥å­¦ä¹ ä»»åŠ¡ç›¸å…³æ›´æ–°ï¼Œè€Œå†»ç»“åŸå§‹å¤§æ¨¡å‹å‚æ•°ã€‚

### 1.1 å®šä¹‰

* å†»ç»“å¤§éƒ¨åˆ†æ¨¡å‹æƒé‡ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°
* æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å’Œè®¡ç®—æˆæœ¬
* å¯å¤ç”¨å¤šä¸ª LoRA adapter æ¥æ”¯æŒå¤šä»»åŠ¡

### 1.2 ç‰¹ç‚¹

* é«˜æ•ˆï¼šè®­ç»ƒå‚æ•°é‡è¿œå°‘äºåŸæ¨¡å‹
* æ˜¾å­˜å‹å¥½ï¼šåªæ›´æ–°ä½ç§©çŸ©é˜µ
* å¯ç»„åˆï¼šä¸åŒä»»åŠ¡å¯å¤ç”¨å¤šä¸ª LoRA adapter

---

## 2. æ•°å­¦åŸç†

å‡è®¾ä¸€ä¸ªçº¿æ€§å±‚æƒé‡çŸ©é˜µ \$W\_0 \in \mathbb{R}^{d \times k}\$ï¼Œåœ¨ LoRA ä¸­ï¼Œæˆ‘ä»¬å°†å¾®è°ƒæ›´æ–°è¡¨ç¤ºä¸ºä½ç§©çŸ©é˜µä¹˜ç§¯ï¼š

$$
\Delta W = A B
$$

å…¶ä¸­ï¼š

* \$A \in \mathbb{R}^{d \times r}\$ï¼Œ\$B \in \mathbb{R}^{r \times k}\$
* \$r \ll \min(d, k)\$ æ˜¯ä½ç§©ç»´åº¦
* \$\alpha\$ æ˜¯ç¼©æ”¾ç³»æ•°

æœ€ç»ˆæƒé‡ï¼š

$$
W = W_0 + \alpha \Delta W = W_0 + \alpha A B
$$

**ç›´è§‚ç†è§£**ï¼š

* \$W\_0\$ æ˜¯åŸå§‹æ¨¡å‹èƒ½åŠ›
* \$A B\$ æ˜¯ä»»åŠ¡é€‚é…å™¨ï¼Œä»…å­¦ä¹ ä»»åŠ¡ç‰¹å®šæ›´æ–°

### 2.1 çº¿æ€§å±‚ä½ç§©è¡¨ç¤º

* å¾®è°ƒå‚æ•°é‡å¤§å¹…å‡å°‘
* ä¿æŒåŸå§‹èƒ½åŠ›ä¸å˜

### 2.2 Transformer ä¸­åº”ç”¨

#### æ³¨æ„åŠ›å±‚

```
Q' = Q0 + Î± * A_Q B_Q
K' = K0 + Î± * A_K B_K
V' = V0 + Î± * A_V B_V
O' = O0 + Î± * A_O B_O
```

#### å‰é¦ˆå±‚ (FFN)

```
FFN_out = FFN0(x) + Î± * A_FFN B_FFN * x
```

> æ‰€æœ‰åŸå§‹æƒé‡ä¿æŒå†»ç»“ï¼Œåªè®­ç»ƒä½ç§©çŸ©é˜µ A å’Œ B

---

## 3. ä¼˜åŠ¿ä¸é€‚ç”¨åœºæ™¯

* **å‚æ•°é«˜æ•ˆ**ï¼šå¾®è°ƒå‚æ•°è¿œå°‘äºåŸæ¨¡å‹å‚æ•°
* **æ˜¾å­˜å‹å¥½**ï¼šåªè®¡ç®—ä½ç§©çŸ©é˜µæ¢¯åº¦
* **å¯å¤ç”¨**ï¼šä¸åŒä»»åŠ¡ä¿å­˜ä¸åŒ LoRA adapter
* **æ˜“äºç»„åˆ**ï¼šæ”¯æŒå¤šä»»åŠ¡å¤šæ¨¡æ€ä»»åŠ¡åˆ‡æ¢

---

## 4. LoRA å¾®è°ƒæµç¨‹

### 4.1 æµç¨‹ç¤ºæ„

```mermaid
flowchart LR
  Input[è¾“å…¥ Token Embedding] --> Attention[Transformer Attention Layer]
  Attention --> FFN[Feed-Forward Layer]
  FFN --> Output[è¾“å‡º Token]

  subgraph LoRA_Adapters
    LA_QV[Low-rank Q/K/V]
    LA_O[Low-rank O]
    LA_FFN[Low-rank FFN]
  end

  Attention --> LA_QV
  Attention --> LA_O
  FFN --> LA_FFN

  LA_QV --> Attention
  LA_O --> Attention
  LA_FFN --> FFN

  classDef smallFont font-size:12px, padding:6px;
  class Input,Attention,FFN,Output,LA_QV,LA_O,LA_FFN smallFont;
```

### 4.2 Python ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=4, alpha=1.0):
        super().__init__()
        self.weight = nn.Parameter(torch.zeros(out_features, in_features))  # frozen weight
        self.A = nn.Parameter(torch.randn(out_features, r) * 0.01)
        self.B = nn.Parameter(torch.randn(r, in_features) * 0.01)
        self.alpha = alpha
        self.r = r

    def forward(self, x):
        delta = self.alpha * (self.A @ self.B)
        W = self.weight + delta  # weight is frozen, delta is trainable
        return x @ W.T

# ç¤ºä¾‹ï¼šå¾®è°ƒ Transformer çš„ Q/K/V çŸ©é˜µ
q_lora = LoRALinear(1024, 1024, r=8, alpha=16)
x = torch.randn(2, 10, 1024)
out = q_lora(x)
```

---

## 5. åœ¨å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„åº”ç”¨

### 5.1 è¾“å…¥æŠ•å½±å±‚å¾®è°ƒ

* LoRA å¯å¾®è°ƒ **input projector**
* æå‡æ¨¡æ€ç‰¹å¾å¯¹é½èƒ½åŠ›

### 5.2 è¾“å‡ºæŠ•å½±å±‚å¾®è°ƒ

* LoRA å¯è®­ç»ƒ **output projector**
* æ”¹å–„è§£ç å™¨æ¡ä»¶ä¿¡å·

### 5.3 LLM ä¸»å¹²å¾®è°ƒ

* LoRA é™„åŠ åˆ°æ³¨æ„åŠ›å’Œ FFN å±‚
* åŸå§‹ LLM å‚æ•°å†»ç»“

### 5.4 å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒ

* åœ¨å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®ä¸Šå¾®è°ƒ LoRA
* å¯åœ¨ä¿æŒ projector å›ºå®šçš„å‰æä¸‹ï¼Œè®© LLM å­¦ä¼šéµå¾ªæŒ‡ä»¤

---

## 6. å®è·µå»ºè®®

* **ä½ç§© r**ï¼š4\~16ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©
* **ç¼©æ”¾ Î±**ï¼šä¸ r æˆæ­£æ¯”
* **å†»ç»“åŸæ¨¡å‹**ï¼šèŠ‚çœæ˜¾å­˜
* **å¯å¤ç”¨ Adapter**ï¼šä¸åŒä»»åŠ¡ã€ä¸åŒæ¨¡æ€ä½¿ç”¨ä¸åŒ LoRA
* **ç»“åˆ PEFT æ¡†æ¶**ï¼šHuggingFace PEFTã€PaddleNLP PEFT

---


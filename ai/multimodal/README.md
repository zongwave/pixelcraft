
---

# ğŸ“‘ å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯æ€»ç»“

## ç›®å½•

* [1. æ ¸å¿ƒæ¶æ„ä¸åŸç†](#1-æ ¸å¿ƒæ¶æ„ä¸åŸç†)

  * [1.1 è¾“å…¥å¤„ç†å±‚](#11-è¾“å…¥å¤„ç†å±‚)

    * [1.1.1 å¤šæ¨¡æ€ç¼–ç å™¨](#111-å¤šæ¨¡æ€ç¼–ç å™¨)
    * [1.1.2 è¾“å…¥æŠ•å½±å±‚](#112-è¾“å…¥æŠ•å½±å±‚)
  * [1.2 æ ¸å¿ƒéª¨å¹²ç½‘ç»œ](#12-æ ¸å¿ƒéª¨å¹²ç½‘ç»œ)

    * [1.2.1 å¤§è¯­è¨€æ¨¡å‹](#121-å¤§è¯­è¨€æ¨¡å‹)
  * [1.3 è¾“å‡ºç”Ÿæˆå±‚](#13-è¾“å‡ºç”Ÿæˆå±‚)

    * [1.3.1 è¾“å‡ºæŠ•å½±å±‚](#131-è¾“å‡ºæŠ•å½±å±‚)
    * [1.3.2 å¤šæ¨¡æ€è§£ç å™¨](#132-å¤šæ¨¡æ€è§£ç å™¨)

* [2. è®­ç»ƒä¸å¾®è°ƒç­–ç•¥](#2-è®­ç»ƒä¸å¾®è°ƒç­–ç•¥)

  * [2.1 é˜¶æ®µä¸€ æ¨¡æ€å¯¹é½é¢„è®­ç»ƒ](#21-é˜¶æ®µä¸€-æ¨¡æ€å¯¹é½é¢„è®­ç»ƒ)
  * [2.2 é˜¶æ®µäºŒ è¾“å‡ºå¯¹é½é¢„è®­ç»ƒ](#22-é˜¶æ®µäºŒ-è¾“å‡ºå¯¹é½é¢„è®­ç»ƒ)
  * [2.3 é˜¶æ®µä¸‰ æŒ‡ä»¤å¾®è°ƒ](#23-é˜¶æ®µä¸‰-æŒ‡ä»¤å¾®è°ƒ)

* [3. å…³é”®å®ç°ç»†èŠ‚ä¸æŒ‘æˆ˜](#3-å…³é”®å®ç°ç»†èŠ‚ä¸æŒ‘æˆ˜)

  * [3.1 å‚æ•°é«˜æ•ˆå¾®è°ƒ PEFT](#31-å‚æ•°é«˜æ•ˆå¾®è°ƒ-peft)
  * [3.2 æ¨ç†ä¸éƒ¨ç½²ä¼˜åŒ–](#32-æ¨ç†ä¸éƒ¨ç½²ä¼˜åŒ–)
  * [3.3 æŒ‘æˆ˜](#33-æŒ‘æˆ˜)

---

## 1. æ ¸å¿ƒæ¶æ„ä¸åŸç†

å¤šæ¨¡æ€å¤§æ¨¡å‹é€šå¸¸é‡‡ç”¨ **â€œç¼–ç å™¨-éª¨å¹²-è§£ç å™¨â€** (Encoder-Backbone-Decoder) çš„é€šç”¨èŒƒå¼ã€‚Next-GPT æ˜¯è¯¥èŒƒå¼çš„å…¸å‹ä»£è¡¨ã€‚

```mermaid
flowchart TD
  %% ===== Overall dataflow =====
  subgraph UserInput
    A1[Prompt<br>with image, video, audio tokens]
  end

  subgraph Preprocessing
    A2[Tokenizer,<br>replace special tokens]
    A3[Conversation<br>construction]
  end

  subgraph Encoder
    B1[Image / Video / Audio<br>input]
    B2[Extract modality<br>embeddings via ImageBind]
  end

  subgraph InputProjector
    B3[mm_input_projector<br>linear or group]
    B4[Project to LLM<br>embedding space]
  end

  subgraph LLM
    C1[Transformer decoder<br>Vicuna / LLaMA]
    C2[LoRA adapters<br>via PEFT]
    C3[Combine text and<br>multimodal embeddings]
  end

  subgraph OutputProjector
    D1[mm_output_projector<br>transformer]
    D2[Produce fixed length<br>query tokens]
    D3[image 77 / video 24 / audio 8]
  end

  subgraph Decoders
    E1[Stable Diffusion<br>image]
    E2[ZeroScope<br>video]
    E3[AudioLDM<br>audio]
  end

  %% connections
  A1 --> A2 --> A3 --> C1
  B1 --> B2 --> B3 --> B4 --> C1
  C1 --> C2 --> C3 --> D1
  D1 --> D2 --> D3
  D3 --> E1
  D3 --> E2
  D3 --> E3

  classDef smallFont font-size:12px, padding:6px;
  class A1,A2,A3,B1,B2,B3,B4,C1,C2,C3,D1,D2,D3,E1,E2,E3 smallFont;
```

---

### 1.1 è¾“å…¥å¤„ç†å±‚

#### 1.1.1 å¤šæ¨¡æ€ç¼–ç å™¨

* **å›¾åƒç¼–ç å™¨**ï¼šCLIP ViT æˆ– BLIP-2 Q-Former
  åŠŸèƒ½ï¼šå°†å›¾åƒç¼–ç ä¸ºå›ºå®šæ•°é‡ token ç‰¹å¾ `V = [v1, ..., vN]`

* **éŸ³é¢‘ç¼–ç å™¨**ï¼šImageBindã€CLAP æˆ– Whisper
  åŠŸèƒ½ï¼šå°†éŸ³é¢‘æ³¢å½¢ç¼–ç ä¸ºåºåˆ—ç‰¹å¾ `A_enc = [a1, ..., aM]`

* **è§†é¢‘ç¼–ç å™¨**ï¼šå‡åŒ€é‡‡æ ·å¸§å¹¶ä½¿ç”¨å›¾åƒç¼–ç å™¨ï¼ŒéŸ³é¢‘è½¨å¯å•ç‹¬ç¼–ç ï¼Œå†èåˆè§†è§‰å’ŒéŸ³é¢‘ç‰¹å¾

#### 1.1.2 è¾“å…¥æŠ•å½±å±‚

* åŠŸèƒ½ï¼šå°†æ¨¡æ€ç‰¹å¾ `(V, A_enc, ...)` æŠ•å½±åˆ° LLM çš„åµŒå…¥ç©ºé—´
* å®ç°ï¼šçº¿æ€§å±‚ (`nn.Linear`) æˆ–ç»„çº¿æ€§
* é‡è¦æ€§ï¼šå®ç°æ¨¡æ€å¯¹é½ï¼Œä½¿ LLM ç†è§£éæ–‡æœ¬æ¨¡æ€

---

### 1.2 æ ¸å¿ƒéª¨å¹²ç½‘ç»œ

#### 1.2.1 å¤§è¯­è¨€æ¨¡å‹

* æ¨¡å‹ï¼šLLaMAã€Vicunaã€Qwen
* åŠŸèƒ½ï¼šæ¥æ”¶æ–‡æœ¬ token + æŠ•å½±åçš„éæ–‡æœ¬ token
* è¾“å…¥æ ¼å¼ï¼š
  `[Text_Tokens] + [IMG] + [Projected_Image_Tokens] + ...`
  ä½¿ç”¨ `<Image>`ã€`<Audio>` ç­‰ç‰¹æ®Š token æŒ‡ç¤ºæ¨¡æ€

---

### 1.3 è¾“å‡ºç”Ÿæˆå±‚

#### 1.3.1 è¾“å‡ºæŠ•å½±å±‚

* åŠŸèƒ½ï¼šå°† LLM hidden states è½¬æ¢ä¸ºè§£ç å™¨ query token
* å®ç°ï¼šçº¿æ€§å±‚

#### 1.3.2 å¤šæ¨¡æ€è§£ç å™¨

* æ–‡æœ¬è§£ç å™¨ï¼šLLM æœ¬èº«
* å›¾åƒè§£ç å™¨ï¼šæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼Œå¦‚ Stable Diffusionï¼‰
* éŸ³é¢‘/è§†é¢‘è§£ç å™¨ï¼šAudioLDMã€VideoLDM æˆ– CogVideo

---

## 2. è®­ç»ƒä¸å¾®è°ƒç­–ç•¥

ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š

```mermaid
flowchart LR
  %% ===== Pretrain Encoder =====
  P1[Datasets<br>cc3m / webvid / audiocap] --> P2[ImageBind encoder]
  P2 --> P3[mm_input_projector<br>TRAIN]
  P3 --> P4[LLM<br>FROZEN]
  P4 --> P5[Optional gen losses<br>via decoders FROZEN]

  %% ===== Pretrain Decoder =====
  Q1[Datasets<br>with caption embeddings] --> Q2[LLM<br>FROZEN]
  Q2 --> Q3[mm_output_projector<br>TRAIN selected mods]
  Q3 --> Q4[Decoders<br>FROZEN]

  %% ===== Instruction Fine-tune =====
  R1[Instruction datasets<br>image / video / audio / text] --> R2[LLM with LoRA<br>TRAIN]
  R2 --> R3[mm_input_projector<br>FROZEN pretrained]
  R2 --> R4[mm_output_projector<br>FROZEN pretrained]

  classDef smallFont font-size:12px, padding:6px;
  class P1,P2,P3,P4,P5,Q1,Q2,Q3,Q4,R1,R2,R3,R4 smallFont;
```

### 2.1 é˜¶æ®µä¸€ æ¨¡æ€å¯¹é½é¢„è®­ç»ƒ

* è„šæœ¬ï¼š`pretrain_enc.sh`
* å¯¹è±¡ï¼šè¾“å…¥æŠ•å½±å±‚
* å†»ç»“ï¼šLLMã€è¾“å‡ºæŠ•å½±å±‚ã€è§£ç å™¨
* ç›®æ ‡ï¼šå°†æ¨¡æ€ embedding ç¿»è¯‘ä¸º LLM å¯ç†è§£çš„è¯­è¨€

### 2.2 é˜¶æ®µäºŒ è¾“å‡ºå¯¹é½é¢„è®­ç»ƒ

* è„šæœ¬ï¼š`pretrain_dec.sh`
* å¯¹è±¡ï¼šè¾“å‡ºæŠ•å½±å±‚
* å†»ç»“ï¼šLLMã€è§£ç å™¨
* ç›®æ ‡ï¼šå°† LLM hidden states ç¿»è¯‘ä¸ºè§£ç å™¨ query token

### 2.3 é˜¶æ®µä¸‰ æŒ‡ä»¤å¾®è°ƒ

* è„šæœ¬ï¼š`train_mem.sh`
* å¯¹è±¡ï¼šLLMï¼ˆLoRA å¾®è°ƒï¼‰
* å†»ç»“ï¼šè¾“å…¥/è¾“å‡ºæŠ•å½±å±‚
* ç›®æ ‡ï¼šå­¦ä¼šå¤šæ¨¡æ€æŒ‡ä»¤å¯¹è¯ä¸ç”Ÿæˆ

---

## 3. å…³é”®å®ç°ç»†èŠ‚ä¸æŒ‘æˆ˜

### 3.1 å‚æ•°é«˜æ•ˆå¾®è°ƒ PEFTï¼ˆLoRAï¼‰

**LoRA**ï¼šä»…è®­ç»ƒä½ç§©çŸ©é˜µé€‚é…å™¨ï¼Œå†»ç»“åŸå§‹ LLM æƒé‡ã€‚

#### åŸç†

```
Î”W = A B,  W = W0 + Î± Î”W
```

* \$A \in R^{dÃ—r}\$, \$B \in R^{rÃ—k}\$, \$r \ll min(d,k)\$
* Î± ä¸ºç¼©æ”¾ç³»æ•°

#### Transformer åº”ç”¨

* æ³¨æ„åŠ›å±‚ Q/K/V/Oï¼š`Q' = Q0 + Î± * A_Q B_Q` ç­‰
* FFN å±‚ï¼š`FFN_out = FFN0(x) + Î± * A_FFN B_FFN * x`

#### Python ç¤ºä¾‹

```python
import torch
import torch.nn as nn

class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=4, alpha=1.0):
        super().__init__()
        self.weight = nn.Parameter(torch.zeros(out_features, in_features))
        self.A = nn.Parameter(torch.randn(out_features, r) * 0.01)
        self.B = nn.Parameter(torch.randn(r, in_features) * 0.01)
        self.alpha = alpha

    def forward(self, x):
        delta = self.alpha * (self.A @ self.B)
        W = self.weight + delta
        return x @ W.T

q_lora = LoRALinear(1024, 1024, r=8, alpha=16)
x = torch.randn(2, 10, 1024)
out = q_lora(x)
```


#### Next-GPT åº”ç”¨

1. è¾“å…¥æŠ•å½±å±‚ï¼šå¾®è°ƒ input projector
2. è¾“å‡ºæŠ•å½±å±‚ï¼šå¾®è°ƒ output projector
3. LLM æœ¬ä½“ï¼šé™„åŠ  LoRA åˆ°æ³¨æ„åŠ›å’Œ FFN å±‚
4. å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒï¼šé«˜æ•ˆé€‚é…ä¸åŒä»»åŠ¡

### 3.2 æ¨ç†ä¸éƒ¨ç½²ä¼˜åŒ–

* é™æ€å›¾ç¼–è¯‘ï¼ˆTorchScript/ONNXï¼‰
* ç®—å­èåˆï¼ˆLinear + LayerNormï¼‰
* æƒé‡é‡åŒ–ï¼ˆFP16 â†’ INT8/INT4ï¼‰
* KV-Cache åŠ é€Ÿè‡ªå›å½’ç”Ÿæˆ

### 3.3 æŒ‘æˆ˜

* æ¨¡æ€å¯¹é½
* è®­ç»ƒç¨³å®šæ€§
* è®¡ç®—æˆæœ¬
* ç”Ÿæˆè´¨é‡ä¸å¯æ§æ€§

---

# ğŸ“‘ å¤šæ¨¡æ€å¤§æ¨¡å‹æŠ€æœ¯æ€»ç»“ï¼ˆå« LoRA Adapter æµç¨‹å›¾ï¼‰

---

```mermaid
flowchart TD
  %% ===== User Input & Preprocessing =====
  subgraph UserInput
    A1[Prompt<br>with image, video, audio tokens]
  end

  subgraph Preprocessing
    A2[Tokenizer,<br>replace special tokens]
    A3[Conversation<br>construction]
  end

  %% ===== Encoder & Input Projection =====
  subgraph Encoder
    B1[Image / Video / Audio<br>input]
    B2[Extract modality<br>embeddings via ImageBind]
  end

  subgraph InputProjector
    B3[mm_input_projector<br>linear or group]
    B4[Project to LLM<br>embedding space]
  end

  %% ===== LLM + LoRA =====
  subgraph LLM
    C1[Transformer decoder<br>Vicuna / LLaMA]
    C1A[Attention Q/K/V/O]
    C1B[FFN Layer]
    C3[Combine text &<br>multimodal embeddings]
  end

  %% ===== LoRA Adapters for each modality =====
  subgraph LoRA_Adapters
    LA_IMG[Image LoRA Î”W]
    LA_VID[Video LoRA Î”W]
    LA_AUD[Audio LoRA Î”W]
    LA_TEXT[Text LoRA Î”W]
    W0[Original W0 frozen]
  end

  %% Connections LoRA
  C1A --> LA_IMG
  C1A --> LA_VID
  C1A --> LA_AUD
  C1A --> LA_TEXT
  C1B --> LA_IMG
  C1B --> LA_VID
  C1B --> LA_AUD
  C1B --> LA_TEXT

  LA_IMG --> C1A
  LA_VID --> C1A
  LA_AUD --> C1A
  LA_TEXT --> C1A
  LA_IMG --> C1B
  LA_VID --> C1B
  LA_AUD --> C1B
  LA_TEXT --> C1B

  W0 -.-> C1A
  W0 -.-> C1B

  %% ===== Output Projection =====
  subgraph OutputProjector
    D1[mm_output_projector<br>transformer]
    D2[Produce fixed length<br>query tokens]
    D3[image 77 / video 24 / audio 8]
  end

  %% ===== Decoders =====
  subgraph Decoders
    E1[Stable Diffusion<br>image]
    E2[ZeroScope<br>video]
    E3[AudioLDM<br>audio]
  end

  %% ===== Connections =====
  A1 --> A2 --> A3 --> C1
  B1 --> B2 --> B3 --> B4 --> C1
  C1 --> C3 --> D1
  D1 --> D2 --> D3
  D3 --> E1
  D3 --> E2
  D3 --> E3

  %% ===== Three-stage training =====
  %% Stage 1: Pretrain Encoder
  P1[Datasets<br>cc3m / webvid / audiocap] --> P2[ImageBind encoder]
  P2 --> P3[mm_input_projector<br>TRAIN]
  P3 --> P4[LLM<br>FROZEN]
  P4 --> P5[Optional gen losses<br>via decoders FROZEN]

  %% Stage 2: Pretrain Decoder
  Q1[Datasets<br>with caption embeddings] --> Q2[LLM<br>FROZEN]
  Q2 --> Q3[mm_output_projector<br>TRAIN selected mods]
  Q3 --> Q4[Decoders<br>FROZEN]

  %% Stage 3: Instruction Fine-tune
  R1[Instruction datasets<br>image / video / audio / text] --> R2[LLM with LoRA<br>TRAIN Î”W only]
  R2 --> R3[mm_input_projector<br>FROZEN pretrained]
  R2 --> R4[mm_output_projector<br>FROZEN pretrained]

  %% ===== Style: Small font & padding =====
  classDef smallFont font-size:12px, padding:6px;
  class A1,A2,A3,B1,B2,B3,B4,C1,C1A,C1B,C3,D1,D2,D3,E1,E2,E3,LA_IMG,LA_VID,LA_AUD,LA_TEXT,W0,P1,P2,P3,P4,P5,Q1,Q2,Q3,Q4,R1,R2,R3,R4 smallFont;
```

---

### ğŸ’¡ è¯´æ˜

1. **LoRA Adapter æ¨¡æ€**

   * `LA_IMG`ï¼šå›¾åƒ LoRA Î”W
   * `LA_VID`ï¼šè§†é¢‘ LoRA Î”W
   * `LA_AUD`ï¼šéŸ³é¢‘ LoRA Î”W
   * `LA_TEXT`ï¼šæ–‡æœ¬ LoRA Î”W

2. **è®­ç»ƒé˜¶æ®µ**

   * é˜¶æ®µä¸€ï¼šè¾“å…¥æŠ•å½±è®­ç»ƒï¼ˆå†»ç»“ LLM ä¸è¾“å‡ºæŠ•å½±ï¼‰
   * é˜¶æ®µäºŒï¼šè¾“å‡ºæŠ•å½±è®­ç»ƒï¼ˆå†»ç»“ LLMï¼‰
   * é˜¶æ®µä¸‰ï¼šLLM å¾®è°ƒï¼ˆåªæ›´æ–°å„æ¨¡æ€ LoRA Î”Wï¼ŒåŸå§‹æƒé‡å†»ç»“ï¼‰

3. **å¯è§†åŒ–æ•ˆæœ**

   * å·¦ï¼šæ•°æ®æµå…¥
   * ä¸­ï¼šLLM + LoRA Î”W æŒ‰æ¨¡æ€ç‹¬ç«‹
   * å³ï¼šè¾“å‡ºç”Ÿæˆå™¨
   * LoRA æ’å…¥ç‚¹å’Œè®­ç»ƒçŠ¶æ€æ¸…æ™°

---



# 📑 目录

- 🔧 [一、算子支持完善：补齐 HPU 后端的关键基础算子](#一算子支持完善补齐-hpu-后端的关键基础算子)
- 📊 [二、推理性能分析：性能 Profiling 定位瓶颈与空洞](#二推理性能分析性能-profiling-定位瓶颈与空洞)
- 🔒 [三、Tensor 静态化：避免动态 Shape 编译图路径](#三tensor-静态化避免动态-shape-编译图路径)
- 🤖 [模型结构回顾：LLaMA 推理框架](#模型结构回顾llama-推理框架)
- 🔥 [四、模块级融合：构建高性能 FusedOp（例如 Attention / MLP）](#四模块级融合构建高性能-fusedop例如-attention--mlp)
- 📋 [五、构建静态图执行流：实现全图一体化优化](#五构建静态图执行流实现全图一体化优化)
- 🧮 [六、参数量化优化：从 BF16 动态量化至 Float8](#六参数量化优化从-bf16-动态量化至-float8)
- 🏁 [总结：优化“六步走”策略总览](#总结优化六步走策略总览)



# PaddlePaddle 上部署优化 LLaMA 模型至 Gaudi HPU 的技术要点

 **目标**：实现高性能、全静态图、全算子部署于 HPU 上的大模型推理（以 LLaMA 为代表），显著减少 Fallback 现象，提升计算利用率。

---

## 一、算子支持完善：补齐 HPU 后端的关键基础算子

- **背景问题**：部分 Paddle 算子在 HPU 上无对应实现，触发 fallback 到 CPU，严重影响性能。
- **优化动作**：
  - 分析 Transformer 模型中所有 fallback 算子，逐一注册支持，如：
    - `index_copy`、`reshape`、`softmax`（特定 axis）、`scale`、`cast`、`matmul_v2` 等
  - 使用 `set_output` 和 `infer_shape` 等 API 完善 kernel 注册逻辑
- **工具辅助**：
  - 使用 `PADDLE_DEBUG=1` 或 `GLOG_v=3` 查看 fallback 情况

---

## 二、推理性能分析：性能 Profiling 定位瓶颈与空洞

- **目标**：识别推理执行中出现“空洞”（低算子利用率或 memory-bound 问题）的阶段。
- **方法**：
  - 使用 Habana Profiler 抓取运行 profiling log
  - 使用 chrome://tracing/ 打开log，分析每个 op 的执行时间与等待时间
  - 可视化算子时间线，观察 kernel 执行不连续或频繁数据传输的区域
- **结果输出**：
  - 发现瓶颈点，指导后续 fuse 和调度顺序优化

---

## 三、Tensor 静态化：避免动态 Shape 编译图路径

- **问题**：HPU 执行图不支持动态 shape 会触发 fallback 或触发 compile cache miss。
- **优化方式**：
  - 将输入及中间张量维度固定，如 batch size、seq_len、hidden size
  - 对动态 axis 使用 padding 保证形状统一
  - 使用 shape hint 或提前编译静态图结构
- **调试方法**：
  - 利用 shape trace 工具确认执行路径是否存在动态 shape

---

## 模型结构回顾：LLaMA 推理框架

为了更清晰理解后续的融合优化策略，我们回顾 LLaMA 解码器的结构：

[![LLaMA 推理结构示意图](diagram/llama_decoder.png)](https://raw.githubusercontent.com/zongwave/pixelcraft/main/ai/llm/infra_opt/diagram/llama_decoder.png)

> *图：LLaMA 解码器结构图，展示 RMSNorm、Self-Attention、MLP 的典型堆叠顺序。*

---

## 四、模块级融合：构建高性能 FusedOp（例如 Attention / MLP）

- **目的**：减少 kernel launch 次数与中间 memory I/O，提升整体吞吐。
- **典型融合策略**：
  - **MLP 模块**：融合 `linear1 + activation + linear2 + dropout + linear3`
  - **LayerNorm 融合**：将前置 `rmsnorm` 与 MLP 一起融合执行
  - **Attention 融合**：融合 `qkv projection + reshape + matmul + softmax + matmul + output projection`
- **实现方式**：
  - 编写 `FusedKernel` 并对外注册为自定义 OP
  - 利用静态 shape 支持重用 memory buffer，避免中间张量重复分配

---

## 五、构建静态图执行流：实现全图一体化优化

- **目标**：从动态图切换为完整静态图，提高编译与运行效率
- **实现方式**：
  - 使用 `paddle.jit.to_static` 或手动构建 `ProgramDesc`
  - 禁用 Eager 模式下的缓存与调试信息
  - 加入图级 Pass，如：
    - `inplace_fuse_pass`
    - `pre_op_fuse_pass`
- **优势**：
  - 更好利用 HPU 编译器图优化
  - 提高推理图缓存命中率

---


### 1. **算子融合（Operator Fusion）**

* **原理**：把多个连续的算子（如卷积 + 激活）合并成一个更大、更复杂的算子，减少中间数据的读写和调度开销。
* **效果**：减少kernel调用次数，降低内存访问，提升计算密集度和硬件利用率。
* **举例**：卷积层后面紧跟激活层，融合成一个卷积激活算子一起执行。

---

### 2. **内存复用与预分配**

* **原理**：静态图在执行前全局分析各算子间的内存依赖关系，提前规划内存布局，复用计算图中不同阶段不重叠使用的内存区域。
* **效果**：减少动态内存分配次数和内存峰值，降低内存碎片和延迟。
* **实现**：通过内存池管理和内存重用机制，减少内存申请释放开销。

---

### 3. **图优化与裁剪**

* **原理**：对静态计算图进行分析，去除无用节点、合并重复计算、调整计算顺序，简化图结构。
* **效果**：减少不必要的计算，降低计算量和执行时间。
* **举例**：删除恒定节点或未使用的中间变量。

---

### 4. **静态调度**

* **原理**：计算图构建后，调度顺序、数据依赖和资源分配都固定好，运行时无需动态计算依赖关系。
* **效果**：避免运行时的调度开销和动态决策，提升执行效率和预测时延确定性。

---

### 5. **跨算子调度优化**

* **原理**：多个算子调度合并为一批次，减少CPU和设备间频繁交互。
* **效果**：降低调度通信开销，提高设备利用率。

---

### 6. **硬件加速与定制算子**

* **原理**：针对不同硬件平台（CPU、GPU、NPU等），静态图编译时选用最优算子实现，并使用专门优化的库（如MKL、cuDNN）。
* **效果**：充分发挥硬件性能，提升推理吞吐和响应速度。

---

### 7. **模型量化与剪枝**

* **配合静态图**：静态图结构清晰，方便对模型做低精度量化和剪枝，减少模型大小和计算量，进一步提升推理速度。


1. **硬件加速器与 FP8 支持**

   * 现代 AI 加速芯片（如 NVIDIA Hopper TransformerEngine、Intel HPU、Habana Gaudi 等）通常对 GEMM（矩阵乘法）提供 **FP8 精度计算**。
   * 因此，在推理代码中，**与 GEMM 相关的运算可以使用 FP8 算子**，充分利用硬件加速能力，提高性能和吞吐量。

2. **FP8 与高精度计算的结合**

   * 在 Transformer/LLM 中，部分操作（如残差连接、LayerNorm、Softmax 等）对精度敏感。
   * 这些操作通常保留 **FP16/BF16 计算**，以避免精度损失和误差积累。
   * 因此 FP8 算子输出在必要时需要 **反量化（dequantize）到 FP16/BF16**，以供后续高精度计算使用。

3. **用户代码与量化参数**

   * 用户无需手动进行量化或反量化，只需向硬件算子提供：

     1. **数据格式类型**（如 FP8 E4M3、E5M2）
     2. **量化 scale 值**（scale factor）
   * 量化 scale 一般根据输入 tensor 的数值分布动态计算：
     [
     \text{scale} = \frac{\text{amax}}{Q_{\max}}
     ]
     其中：

     * **amax**：张量最大绝对值
     * **Qmax**：FP8 类型的最大可表示有限值（E4M3 ≈ 240，E5M2 ≈ 57344）

4. **量化对象选择**

   * **必须量化**：

     * 所有 GEMM（MatMul）操作，包括 Attention Q/K/V 的计算和 FFN 的线性投影。
     * 这样可充分利用 FP8 精度带来的性能提升。
   * **可量化 / 可选择**：

     * 中间激活值，如某些 FFN 中间层输出、Attention 输出等。
     * 可采用 per-tensor 或 per-channel scale 优化精度。
   * **不量化 / 高精度保留**：

     * 残差连接（Residual Stream）
     * LayerNorm、RMSNorm
     * Softmax/Attention Score

5. **Python 层的透明性**

   * 对上层 Python 使用者来说，量化过程对代码基本透明：

     * 用户只需指定是否启用底层硬件 FP8 加速
     * 模型加载的权重通常为 **预训练 BF16/FP16** 参数
     * 硬件自动完成 FP8 量化 → GEMM → 反量化 pipeline

6. **总结**

   * 核心目标是 **在不损失模型精度的前提下，最大化硬件性能**
   * GEMM 使用 FP8 加速
   * 高精度操作（残差、Norm、Softmax）保持 FP16/BF16
   * Scale 动态计算确保小幅值张量不会被压缩塌缩
   * Python 层无需关心量化细节，只需配置硬件加速参数



---

### 总结

Paddle 静态图推理通过**算子融合、内存复用、静态调度、图优化和硬件加速等多重策略**，实现了：

* 减少运行时开销和调度负担
* 降低内存占用和内存管理复杂度
* 提升计算效率和硬件利用率
* 保证推理过程稳定、高效且可跨平台部署



---

## 六、参数量化优化：从 BF16 动态量化至 Float8

- **目标**：减少内存带宽、加快计算速度，兼顾精度，适配 HPU 支持的低精度格式。
- **核心步骤**：
  1. 分析 BF16 参数范围（min/max），统计缩放比例
  2. 将权重进行 Float8 编码（如 E4M3/E5M2）
  3. 使用支持低精度的 FusedOp 替换传统 MatMul
  4. 采用静态 Shape 保证量化后张量维度不变
- **优势**：
  - 权重显存减少 50%
  - 推理内存带宽下降
  - HPU 上可启用更高级别的编译器图融合
- **注意事项**：
  - 精度评估需完整验证（通常 top-1 精度下降 < 1%）
  - 动态量化路径需要保持静态 shape，避免触发 fallback

---

## 总结：优化“六步走”策略总览

| 步骤 | 名称                   | 关键收益                                        |
|------|------------------------|-------------------------------------------------|
| 1    | 完善算子支持           | 避免 CPU fallback，提高设备利用率              |
| 2    | Profiling 分析         | 定位性能瓶颈与算子调度不合理的问题             |
| 3    | Tensor 静态化          | 防止动态 shape 触发 fallback 或编译失败        |
| 4    | Attention / MLP 融合   | 减少 launch 次数与中间数据传输                 |
| 5    | 构建静态图全流程       | 实现图级别优化、最大化编译器性能发挥           |
| 6    | 参数量化（如 Float8）  | 节省内存带宽、加速推理，兼顾精度与效率         |

---

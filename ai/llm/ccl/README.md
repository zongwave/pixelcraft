# ğŸ“‘ CCL æŠ€æœ¯æ€»ç»“ç›®å½•

- [åˆ†å¸ƒå¼é€šä¿¡æ ‡è¯†ç¬¦æ¦‚è§ˆ](#åˆ†å¸ƒå¼é€šä¿¡æ ‡è¯†ç¬¦æ¦‚è§ˆ)
- [ç©ºé—´æ—¶é—´å…³ç³»å›¾](#ç©ºé—´æ—¶é—´å…³ç³»å›¾)
- [Paddle -> HPU é›†ä½“é€šä¿¡æ—¶åºå›¾](#paddle---hpu-é›†ä½“é€šä¿¡æ—¶åºå›¾)
  - [æ—¶åºå›¾æè¿°](#æ—¶åºå›¾æè¿°)
  - [Paddle -> HPU é›†ä½“é€šä¿¡ç®—å­è°ƒç”¨æ—¶åº](#paddle---hpu-é›†ä½“é€šä¿¡ç®—å­è°ƒç”¨æ—¶åº)
- [CCL å¸¸ç”¨å‡½æ•°](#ccl-å¸¸ç”¨å‡½æ•°)

## åˆ†å¸ƒå¼é€šä¿¡æ ‡è¯†ç¬¦æ¦‚è§ˆ

ä»¥ä¸‹è¡¨æ ¼æ€»ç»“äº† `ring_id`ã€`rank_id`ã€`device_id` å’Œ `unique_id` åœ¨ PaddlePaddle åˆ†å¸ƒå¼è®¡ç®—ä¸­çš„å…³ç³»ï¼š

| æ¦‚å¿µ         | å®šä¹‰                                      | ä½œç”¨                                      | ç¤ºä¾‹ï¼ˆåŸºäºæ—¥å¿—ï¼‰                            | ä¸å…¶ä»–æ¦‚å¿µçš„å…³ç³»                                      |
|--------------|-------------------------------------------|-------------------------------------------|---------------------------------------------|-------------------------------------------------------|
| **`ring_id`** | é€šä¿¡ç»„çš„æ ‡è¯†ç¬¦ï¼Œè¡¨ç¤ºä¸€ä¸ªé€šä¿¡ç¯æˆ–å­åŸŸ    | å®šä¹‰é€šä¿¡çš„èŒƒå›´å’Œåˆ†ç»„ï¼Œå…è®¸å¤šä¸ªç‹¬ç«‹é€šä¿¡ç»„ | `ring_id: 14` (mp_allreduce_sum çš„é€šä¿¡ç»„)   | å®šä¹‰ `rank_id` çš„ä½œç”¨èŒƒå›´ï¼Œä¸ `unique_id` å…±åŒç¡®å®šé€šä¿¡åŸŸ |
| **`rank_id`** | é€šä¿¡ç»„å†…è¿›ç¨‹çš„å”¯ä¸€ç¼–å·ï¼Œä» 0 åˆ° nranks-1 | æ ‡è¯†ç»„å†…è¿›ç¨‹çš„èº«ä»½å’Œæ•°æ®åˆ†ç‰‡             | `rank: 0, nranks: 2` (è¿›ç¨‹ 0)<br>`rank: 1` | åœ¨ç‰¹å®š `ring_id` å†…å”¯ä¸€ï¼Œå¯ä¸ `device_id` å¯¹åº”æˆ–ä¸å¯¹åº” |
| **`device_id`**| ç‰©ç†è®¾å¤‡çš„ç¼–å·ï¼Œæ ‡è¯†ç¡¬ä»¶è®¾å¤‡            | æŒ‡å®šè®¡ç®—å’Œé€šä¿¡è¿è¡Œçš„ç‰©ç†ä½ç½®              | `set device id to 2` (HPU 2)<br>`set device id to 3` | ä¸ `rank_id` å¯ä¸€ä¸€å¯¹åº”ï¼Œä¹Ÿå¯å¤šå¯¹ä¸€ï¼Œç‹¬ç«‹äº `ring_id`  |
| **`unique_id`**| å…¨å±€é€šä¿¡åŸŸçš„å”¯ä¸€æ ‡è¯†ç¬¦                  | åˆå§‹åŒ–é€šä¿¡ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿æ‰€æœ‰ rank ä¸€è‡´æ€§    | `unique_id = 90000000040cb56e5c055...`     | è·¨æ‰€æœ‰ `ring_id` å’Œ `rank_id`ï¼Œç»‘å®šæ•´ä¸ªé€šä¿¡ä»»åŠ¡        |


```mermaid
graph TD
    subgraph é€šä¿¡æ ‡è¯†ç¬¦ä½œç”¨èŒƒå›´
        UID["unique_id<br/>ï¼ˆè·¨è¿›ç¨‹å…±äº«æ ‡è¯†ç¬¦<br/>ç”¨äºé€šä¿¡ä¸Šä¸‹æ–‡åˆå§‹åŒ–ï¼‰"]
        Ring14["ring_id: 14<br/>ï¼ˆé€»è¾‘é€šä¿¡ç»„ IDï¼‰"]
        Rank0["rank: 0<br/>device_id: 2"]
        Rank1["rank: 1<br/>device_id: 3"]
    end

    UID --> Ring14
    Ring14 --> Rank0
    Ring14 --> Rank1

    subgraph ç‰©ç†è®¾å¤‡ç»‘å®š
        Dev2["device_id: 2<br/>ï¼ˆç‰©ç†è®¾å¤‡ç¼–å·ï¼‰"]
        Dev3["device_id: 3<br/>ï¼ˆç‰©ç†è®¾å¤‡ç¼–å·ï¼‰"]
    end

    Rank0 --> Dev2
    Rank1 --> Dev3


```

## ç©ºé—´æ—¶é—´å…³ç³»å›¾

ä»¥ä¸‹æ˜¯ `ring_id`ã€`rank_id`ã€`device_id` å’Œ `unique_id` çš„ç©ºé—´å’Œæ—¶é—´å…³ç³»çš„å¯è§†åŒ–è¡¨ç¤ºï¼š

```mermaid
sequenceDiagram
    actor DistributedTask as Task
    participant UniqueID
    participant Ring14 as "ring_id: 14"
    participant Rank0 as "rank: 0<br/>device_id: 2"
    participant Rank1 as "rank: 1<br/>device_id: 3"

    rect rgba(0, 150, 255, 0.1)
    Task ->> UniqueID: ç”Ÿæˆ unique_id<br/>(90000000040cb56e5c055...)
    UniqueID ->> Rank0: åˆ†å‘ unique_id
    UniqueID ->> Rank1: åˆ†å‘ unique_id
    Note right of UniqueID: unique_id ç¡®ä¿å…¨å±€é€šä¿¡ä¸€è‡´æ€§

    Task ->> Ring14: å®šä¹‰é€šä¿¡ç»„
    Ring14 ->> Rank0: åˆå§‹åŒ–<br/>(rank: 0, nranks: 2)
    Ring14 ->> Rank1: åˆå§‹åŒ–<br/>(rank: 1, nranks: 2)
    Note right of Ring14: ring_id: 14 å®šä¹‰é€šä¿¡èŒƒå›´

    Rank0 ->> Rank0: ç»‘å®š device_id: 2
    Rank1 ->> Rank1: ç»‘å®š device_id: 3
    Note right of Rank1: device_id æŒ‡å®šç‰©ç†è®¾å¤‡
    end

    rect rgba(255, 150, 0, 0.1)
    Task ->> Ring14: æ‰§è¡Œ mp_allreduce_sum
    Ring14 ->> Rank0: æ‰§è¡Œ (rank: 0)
    Ring14 ->> Rank1: æ‰§è¡Œ (rank: 1)
    Rank0 ->> Rank1: HCCL é€šä¿¡
    Rank1 ->> Rank0: HCCL é€šä¿¡
    Note right of Rank1: rank_id åœ¨ ring_id å†…åä½œ
    end
```

## Paddle -> HPU é›†ä½“é€šä¿¡æ—¶åºå›¾

ä»¥ä¸‹æ˜¯ PaddlePaddle åœ¨ HPU ä¸Šæ‰§è¡Œé›†ä½“é€šä¿¡ï¼ˆä»¥ `allreduce` ä¸ºä¾‹ï¼‰çš„å®Œæ•´æµç¨‹ï¼ŒåŒ…æ‹¬åˆå§‹åŒ–ã€å‚æ•°è®¾ç½®ã€æ‰§è¡Œæ“ä½œï¼ˆOpï¼‰ä»¥åŠå®Œæˆæ¸…åœºçš„æ—¶ç©ºå›¾æè¿°ã€‚å›¾ä¸­å±•ç¤ºäº†å¤šä¸ª HPU è®¾å¤‡åœ¨æ—¶é—´è½´ä¸Šçš„åä½œè¿‡ç¨‹ã€‚

### æ—¶åºå›¾æè¿°
1. **åˆå§‹åŒ–é˜¶æ®µ**:
   - ç”Ÿæˆå…¨å±€å”¯ä¸€çš„ `unique_id`ï¼Œç”¨äºæ ‡è¯†æ•´ä¸ªé€šä¿¡ä»»åŠ¡ã€‚
   - ä¸ºæ¯ä¸ªé€šä¿¡ç»„åˆ†é… `ring_id`ï¼Œç¡®å®šé€šä¿¡èŒƒå›´ã€‚
   - è®¾ç½®æ¯ä¸ªè¿›ç¨‹çš„ `rank_id` å’Œå¯¹åº”çš„ `device_id`ï¼Œç»‘å®šåˆ°ç‰©ç† HPU è®¾å¤‡ã€‚

2. **å‚æ•°è®¾ç½®é˜¶æ®µ**:
   - é…ç½®é€šä¿¡ç»„å†…çš„è¿›ç¨‹æ•°é‡ï¼ˆ`nranks`ï¼‰å’Œé€šä¿¡ç®—æ³•ï¼ˆå¦‚ Ring AllReduceï¼‰ã€‚
   - æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºå¼ é‡ï¼Œä»¥åŠé€šä¿¡æ“ä½œç±»å‹ï¼ˆä¾‹å¦‚ `sum`ã€`max`ï¼‰ã€‚

3. **æ‰§è¡Œæ“ä½œï¼ˆOpï¼‰é˜¶æ®µ**:
   - å„ HPU æ ¹æ® `ring_id` å’Œ `rank_id` æ‰§è¡Œæ•°æ®åˆ†ç‰‡å’Œé€šä¿¡ã€‚
   - é€šè¿‡ HPU ç¡¬ä»¶åŠ é€Ÿå®Œæˆé›†ä½“é€šä¿¡ï¼ˆå¦‚ `allreduce`ï¼‰ã€‚

4. **å®Œæˆæ¸…åœºé˜¶æ®µ**:
   - åŒæ­¥æ‰€æœ‰ HPUï¼Œç¡®ä¿é€šä¿¡å®Œæˆã€‚
   - é‡Šæ”¾é€šä¿¡ä¸Šä¸‹æ–‡å’Œä¸´æ—¶ç¼“å†²åŒºï¼Œæ¸…ç†èµ„æºã€‚

### Paddle -> HPU é›†ä½“é€šä¿¡ç®—å­è°ƒç”¨æ—¶åº

```mermaid
sequenceDiagram
    actor PaddleExecutor
    participant CCM as "CommContextManager"
    participant XCCL as "XCCLCommContext"
    participant Device as "CustomDevice"
    participant Runtime as "IntelHPURuntime"
    participant HCCL
    participant HPU as "IntelHPU"

    rect rgba(0,150,255,0.1)
    Note over PaddleExecutor, Device: Initialization
    PaddleExecutor ->> Device: CCLCommInitRank(num_ranks, unique_id, rank, &comm)
    Device ->> Runtime: XcclCommInitRank(...)
    Runtime ->> HCCL: hcclCommInitRank(...)
    HCCL -->> Runtime: Return hcclComm_t
    Runtime -->> Device: Return CCLComm
    Device -->> PaddleExecutor: Return CCLComm
    PaddleExecutor ->> CCM: GetOrCreate(ring_id)
    CCM -->> PaddleExecutor: Return XCCLCommContext
    end

    rect rgba(255,150,0,0.1)
    Note over PaddleExecutor, CCM: Communication
    PaddleExecutor ->> CCM: ParseDeviceContext(op, place)
    CCM -->> PaddleExecutor: Return XCCLCommContext
    PaddleExecutor ->> XCCL: AllReduce(data, comm, stream)
    XCCL ->> Device: XcclAllReduce(data, comm, stream)
    Device ->> Runtime: XcclAllReduce(...)
    Runtime ->> HCCL: hcclAllReduce(...)
    HCCL ->> HPU: Execute AllReduce
    HCCL -->> Runtime: Return success
    Runtime -->> Device: Return success
    Device -->> XCCL: Return success
    XCCL -->> PaddleExecutor: Return success
    end

```

## CCL å¸¸ç”¨å‡½æ•°

åœ¨åˆ†å¸ƒå¼æ¨ç†ä¸­ï¼Œ\*\*é›†åˆé€šä¿¡ï¼ˆcollective communicationï¼‰\*\*å‡½æ•°æ˜¯å®ç°å¤šè®¾å¤‡ï¼ˆå¤šå¡ã€å¤šèŠ‚ç‚¹ï¼‰ååŒè®¡ç®—çš„åŸºç¡€ï¼Œå°¤å…¶æ˜¯åœ¨æ¨¡å‹æ‹†åˆ†ï¼ˆå¦‚ Tensor Parallelismï¼‰ã€ä¸“å®¶æ¨¡å‹ï¼ˆå¦‚ MoEï¼‰ã€å¤šå¡æ¨ç†åŠ é€Ÿç­‰åœºæ™¯ä¸­éå¸¸å…³é”®ã€‚ä¸‹é¢æˆ‘å°†åˆ†ä¸‰éƒ¨åˆ†ä»‹ç»ï¼š

---

### ğŸ“Œ ä¸€ã€å¸¸è§é›†åˆé€šä¿¡å‡½æ•°åŠç”¨é€”

| é€šä¿¡å‡½æ•°                | å«ä¹‰                            | ç”¨é€”ä¸é€‚ç”¨åœºæ™¯                                                       |
| ------------------- | ----------------------------- | ------------------------------------------------------------- |
| **broadcast**       | å¹¿æ’­ï¼šä¸€ä¸ªæºè¿›ç¨‹å°†æ•°æ®å‘é€ç»™ç»„å†…æ‰€æœ‰è¿›ç¨‹          | åˆå§‹åŒ–æƒé‡åŒæ­¥ï¼ˆä¾‹å¦‚åŠ è½½ checkpoint åï¼‰ï¼Œæˆ–è€…å›ºå®š master ç”Ÿæˆçš„éšæœºæ•°                 |
| **all\_reduce**     | æ‰€æœ‰è¿›ç¨‹æ‰§è¡Œå½’çº¦æ“ä½œï¼ˆå¦‚æ±‚å’Œã€æ±‚å¹³å‡ï¼‰å¹¶å°†ç»“æœå¹¿æ’­ç»™æ‰€æœ‰äºº | æ¢¯åº¦/æ¿€æ´»æ±‚å’Œï¼ŒTP æ¨¡å—ä¸­è·¨å¡åŠ å’Œï¼ˆå¦‚ KV-cacheã€attention logitsï¼‰               |
| **reduce**          | æ‰€æœ‰è¿›ç¨‹çš„ç»“æœå½’çº¦åˆ°ä¸€ä¸ªç›®æ ‡è¿›ç¨‹              | å°‘ç”¨ã€‚å¯ç”¨äºæ”¶é›†æŸäº›è°ƒè¯•ä¿¡æ¯æˆ–èšåˆä¸­é—´ç»“æœ                                         |
| **all\_gather**     | æ‰€æœ‰è¿›ç¨‹å„è‡ªå‘é€å¼ é‡ï¼Œæœ€ç»ˆæ‰€æœ‰äººæ”¶é›†åˆ°æ‰€æœ‰äººçš„å¼ é‡     | TP/EP åœºæ™¯ä¸­ï¼Œæ”¶é›†éƒ¨åˆ†å¼ é‡ï¼Œå¦‚ token åˆ†ç‰‡åçš„è¾“å‡º                               |
| **gather**          | æ‰€æœ‰è¿›ç¨‹å°†æ•°æ®æ”¶é›†åˆ°ä¸€ä¸ªè¿›ç¨‹                | æ£€æŸ¥ç‚¹ä¿å­˜ã€æ—¥å¿—è®°å½•åœºæ™¯                                                  |
| **scatter**         | ä¸€ä¸ªè¿›ç¨‹å°†ä¸åŒæ•°æ®åˆ†å‘ç»™å„ä¸ªè¿›ç¨‹              | åˆå§‹åŒ–é˜¶æ®µæˆ–æ¨¡å‹åˆ†ç‰‡                                                    |
| **reduce\_scatter** | å…ˆæ‰§è¡Œå½’çº¦æ“ä½œï¼ˆå¦‚ sumï¼‰ï¼Œå†å°†å½’çº¦ç»“æœåˆ†å‘ç»™å„ä¸ªè¿›ç¨‹  | All-Reduce çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¸¸ç”¨äºæ¢¯åº¦åŒæ­¥æˆ–å¹¶è¡Œ attention logits åˆå¹¶               |
| **all\_to\_all**    | æ‰€æœ‰è¿›ç¨‹ä¹‹é—´ç›¸äº’å‘é€æ•°æ®ï¼ˆä»»æ„æ˜ å°„ï¼‰            | MoE çš„æ ¸å¿ƒï¼štoken å’Œ expert åˆ†é…æ—¶çš„ token shuffleã€expert parallel çš„é€šä¿¡ |

---

### ğŸ“Œ äºŒã€å¸¸è§ä½¿ç”¨åœºæ™¯ä¸æ—¶æœº

| åœºæ™¯                       | ä½¿ç”¨çš„é€šä¿¡å‡½æ•°                                      | æ—¶æœº                                 | è¯´æ˜                           |
| ------------------------ | -------------------------------------------- | ---------------------------------- | ---------------------------- |
| **æ¨¡å‹åˆå§‹åŒ–**                | broadcast                                    | æ¨ç†å¯åŠ¨ååŒæ­¥å‚æ•°                          | ä¿è¯æ¯å¼ å¡æ¨¡å‹ä¸€è‡´ï¼Œå°¤å…¶åœ¨ checkpoint åŠ è½½å |
| **Tensor Parallel (TP)** | all\_reduce / reduce\_scatter / all\_gather  | attention/logits å¤„ç†ã€KV åˆå¹¶ã€çº¿æ€§å±‚åˆ‡åˆ†ååˆå¹¶ | TP å¯¹æ¿€æ´»å’Œæƒé‡åˆ‡ç‰‡ï¼Œéœ€è¦é€šä¿¡åˆå¹¶æˆ–åˆ†å‘        |
| **Expert Parallel (EP)** | all\_to\_all / all\_gather / reduce\_scatter | MoE æ¨¡å‹ä¸­ token åˆ†å‘åˆ° expertï¼Œæˆ–æ”¶å›ç»“æœ     | é€šå¸¸éœ€è¦ token -> expert çš„åˆ†é…ä¸äº¤æ¢  |
| **Data Parallel (DP)**   | all\_reduce                                  | æ¢¯åº¦åŒæ­¥ï¼ˆè®­ç»ƒï¼‰                           | æ¨ç†ä¸­ä¸»è¦ç”¨äºçŠ¶æ€ä¸€è‡´æ€§ï¼ŒDPç”¨å¾—è¾ƒå°‘          |
| **KV Cache åˆå¹¶**          | all\_gather æˆ– all\_reduce                    | åˆ†å¸ƒå¼ attention ä¸­å¤šä¸ª head çš„ cache åˆå¹¶  | å¿…é¡»ä¿è¯ä¸Šä¸‹æ–‡ä¸€è‡´æ€§                   |
| **è¾“å‡ºæ‹¼æ¥**                 | all\_gather / gather                         | æ¨ç†è¾“å‡ºé˜¶æ®µ                             | å¤šå¡ç”Ÿæˆçš„ token ä¸²éœ€æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬        |

---

### ğŸ“Œ ä¸‰ã€å®é™…åº“ä¸å‡½æ•°æ¥å£ï¼ˆç¤ºä¾‹ï¼‰

å„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorchã€Paddleã€DeepSpeedï¼‰éƒ½æä¾›äº†é›†åˆé€šä¿¡æ¥å£ã€‚

#### âœ… PyTorch ç¤ºä¾‹ï¼ˆtorch.distributedï¼‰

```python
import torch.distributed as dist

# all_reduce: ç”¨äºåŒæ­¥æ¿€æ´»æˆ–æ¢¯åº¦
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

# all_gather: åˆ†å¸ƒå¼æ¨ç†è¾“å‡ºæ”¶é›†
output_list = [torch.empty_like(tensor) for _ in range(world_size)]
dist.all_gather(output_list, tensor)

# all_to_all: MoE ä¸­ token åˆ° expert çš„æ•°æ®äº¤æ¢
dist.all_to_all(output_tensor, input_tensor)
```

#### âœ… Paddle ç¤ºä¾‹ï¼ˆpaddle.distributedï¼‰

```python
import paddle.distributed as dist

# AllReduce
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)

# AllGather
dist.all_gather(tensor_list, tensor)

# AllToAll
dist.all_to_all(out_tensor, in_tensor)
```

---

### ğŸ“Œ å››ã€æ€»ç»“ä¸å»ºè®®

* å¯¹äº **Tensor Parallel**ï¼Œä¸»è¦å…³æ³¨ `all_reduce`ã€`reduce_scatter`ã€‚
* å¯¹äº **Expert Parallelï¼ˆMoEï¼‰**ï¼Œæ ¸å¿ƒæ˜¯ `all_to_all` ä¸ `all_gather`ã€‚
* **æ¨ç†é˜¶æ®µ**ç”±äºä¸æ¶‰åŠå‚æ•°æ›´æ–°ï¼Œä½†ä»éœ€æ¿€æ´»åŒæ­¥ã€è¾“å‡ºåˆå¹¶ç­‰é›†åˆé€šä¿¡ã€‚
* åœ¨é«˜æ€§èƒ½æ¨ç†éƒ¨ç½²ä¸­ï¼Œä½¿ç”¨ **Ring-AllReduce**ã€**NCCL/A2A ä¼˜åŒ–é€šä¿¡åº“** å¯æå¤§æé«˜æ•ˆç‡ã€‚

---
